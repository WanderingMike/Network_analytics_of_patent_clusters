{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b4afbfead2ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 10)\n",
    "\n",
    "### Uploading csv\n",
    "\n",
    "def prep_csv():\n",
    "    global assignee, cpc, foreigncitation, inventor, otherreference, patent, uspatentcitation\n",
    "\n",
    "    assignee = pd.read_csv(\"assignee.csv\")\n",
    "    cpc = pd.read_csv(\"cpc.csv\")\n",
    "    #foreigncitation = pd.read_csv(\"foreigncitation.csv\")\n",
    "    #inventor = pd.read_csv(\"inventor.csv\")\n",
    "    #otherreference = pd.read_csv(\"otherreference.csv\")\n",
    "    #jppatent = pd.read_csv(\"patent.csv\")\n",
    "    uspatentcitation = pd.read_csv(\"uspatentcitation.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "\n",
    "class Analysis():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.divisions = {}\n",
    "        self.category = \"\"\n",
    "\n",
    "    def data(self,cpc,name=None):\n",
    "\n",
    "        self.category = cpc[:4]\n",
    "\n",
    "        df = pd.DataFrame(columns=['patent_number', 'patent_num_combined_citations', 'patent_date',\n",
    "                                   'patent_num_claims', 'inventors', 'assignees', 'cited_patents',\n",
    "                                   'FC3','FC5','FC10','assignees_num_patents','num_subclass','mainclass','age_backward_citation'])\n",
    "\n",
    "        page = 1\n",
    "\n",
    "        while True:\n",
    "\n",
    "            print(\"Downloading page {}\".format(page))\n",
    "            tmp = self.download(page,cpc)\n",
    "            tmp = self.transform(tmp)\n",
    "\n",
    "            df = pd.concat([df, tmp], axis = 0)\n",
    "\n",
    "            page += 1\n",
    "\n",
    "            if total_patent_count < 10000:\n",
    "                break\n",
    "\n",
    "        df.index = np.arange(0, len(df))\n",
    "\n",
    "        if name:\n",
    "            df.to_pickle(name)\n",
    "        return df\n",
    "\n",
    "    def download(self,page,cpc):\n",
    "\n",
    "        global total_patent_count\n",
    "\n",
    "        url = \"https://api.patentsview.org/patents/query\"\n",
    "        data = {\n",
    "            #\"q\": {\"_and\":[{\"cpc_subgroup_id\": cpc},{\"_gte\":{\"patent_date\":\"2016-11-18\"}},{\"_lt\":{\"patent_date\":\"2019-11-18\"}}]},\n",
    "            \"q\": {\"cpc_subgroup_id\": cpc},\n",
    "            #\"q\": {\"cpc_group_id\": cpc},\n",
    "            \"s\": [{\"patent_date\": \"asc\"}],\n",
    "            \"f\": [\"patent_number\",\n",
    "                  \"patent_num_combined_citations\",\n",
    "                  \"patent_date\",\n",
    "                  \"citedby_patent_number\",\n",
    "                  \"citedby_patent_date\",\n",
    "                  \"cited_patent_number\",\n",
    "                  \"cited_patent_date\",\n",
    "                  \"patent_num_claims\",\n",
    "                  \"assignee_sequence\",\n",
    "                  \"assignee_id\",\n",
    "                  \"inventor_last_name\",\n",
    "                  \"assignee_total_num_patents\",\n",
    "                  \"cpc_group_id\",\n",
    "                  \"cpc_subgroup_id\"],\n",
    "            \"o\": {\"per_page\": 10000, \"page\": page}\n",
    "        }\n",
    "\n",
    "        resp = requests.post(url, json=data)\n",
    "        json_data = resp.json()\n",
    "        df = pd.DataFrame(json_data[\"patents\"])\n",
    "        total_patent_count = json_data[\"total_patent_count\"]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, df):\n",
    "\n",
    "        ### Turn patent grant date into usable format\n",
    "        df[\"patent_date\"] = df[\"patent_date\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "\n",
    "        ### Forward citations\n",
    "        def extract_years(x):\n",
    "            return [datetime.strptime(patent[\"citedby_patent_date\"], \"%Y-%m-%d\") for patent in x]\n",
    "\n",
    "        df[\"citedby_patents\"] = df['citedby_patents'].apply(lambda x: [] if x[0][\"citedby_patent_number\"] == None else extract_years(x))\n",
    "\n",
    "        def f(x,num):\n",
    "\n",
    "            return len([patent for patent in x[\"citedby_patents\"] if patent - x[\"patent_date\"] < timedelta(days = num)])\n",
    "\n",
    "        for k,v in {\"FC3\":1095, \"FC5\":1825, \"FC10\": 3650}.items():\n",
    "            df[k] = df.apply(f, num=v, axis = 1)\n",
    "\n",
    "        ### Classification:\n",
    "\n",
    "        df[\"num_subclass\"] = df[\"cpcs\"].apply(lambda x: len(x))\n",
    "\n",
    "        df[\"mainclass\"] = df[\"cpcs\"].apply(lambda x: list(set([patent[\"cpc_group_id\"] for patent in x])))\n",
    "\n",
    "        ### Assignee indicators\n",
    "        def lookup(criteria, data, output):\n",
    "            url = \"https://api.patentsview.org/patents/query\"\n",
    "\n",
    "            data = {\n",
    "                \"q\": {\"_and\": criteria},\n",
    "                \"f\": data,\n",
    "                \"o\": {\"per_page\": 10000}\n",
    "            }\n",
    "\n",
    "            resp = requests.post(url, json=data)\n",
    "            json_data = resp.json()\n",
    "\n",
    "            return json_data[output]\n",
    "\n",
    "        def assignee_knowledge(x):\n",
    "\n",
    "            if x[\"assignees\"][0][\"assignee_sequence\"] != None:\n",
    "                print(\"Retrieving assignee information for patent {}\".format(x.name))\n",
    "                assignees = list()\n",
    "                for assignee in x[\"assignees\"]:\n",
    "                    assignees.append(assignee[\"assignee_id\"])\n",
    "\n",
    "                print(assignees)\n",
    "                tmp = assignee[assignee[\"assignee_id\"] in assignees]\n",
    "                print(tmp)\n",
    "                input()\n",
    "\n",
    "                # Total know-how (TKH)\n",
    "                sum = 0\n",
    "                for assignee in x[\"assignees\"]:\n",
    "                    if assignee[\"assignee_total_num_patents\"]:\n",
    "                        sum += int(assignee[\"assignee_total_num_patents\"])\n",
    "                    else:\n",
    "                        return 0\n",
    "\n",
    "                # Core area know-how (CKH)\n",
    "                criteria = [{\"assignee_id\":assignees},{\"cpc_group_id\":x[\"mainclass\"]}]\n",
    "                ckh = lookup(criteria,[\"patent_number\"],\"total_patent_count\")\n",
    "\n",
    "                # Total technological strength (TTS)\n",
    "                criteria_tts = [{\"assignee_id\":assignees}]\n",
    "                tts_data = lookup(criteria_tts,[\"citedby_patent_number\"],\"patents\")\n",
    "                tts = 0\n",
    "                for patent in tts_data:\n",
    "                    if patent[\"citedby_patents\"][0][\"citedby_patent_number\"] != None:\n",
    "                        tts += len(patent[\"citedby_patents\"])\n",
    "\n",
    "                # Core area technological strength (CTS)\n",
    "                cts_data = lookup(criteria,[\"citedby_patent_number\"],\"patents\")\n",
    "                cts = 0\n",
    "                for patent in cts_data:\n",
    "                    if patent[\"citedby_patents\"][0][\"citedby_patent_number\"] != None:\n",
    "                        cts += len(patent[\"citedby_patents\"])\n",
    "\n",
    "                return sum, ckh, sum-ckh, tts, cts, tts-cts\n",
    "\n",
    "            else:\n",
    "                return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        df[\"assignees_num_patents\"], df[\"core_know_how\"],df[\"peripheral_know_how\"],df[\"total_tech_strength\"],df[\"core_strength\"],df[\"peripheral_strength\"] = zip(*df.apply(assignee_knowledge, axis = 1))\n",
    "\n",
    "        df[\"assignees\"] = df[\"assignees\"].apply(lambda x: len(x) if x[0][\"assignee_sequence\"] != None else 0)\n",
    "\n",
    "        for column in [\"assignees_num_patents\",\"core_know_how\",\"peripheral_know_how\",\"total_tech_strength\",\"core_strength\",\"peripheral_strength\"]:\n",
    "            df[column] = df[column].replace(np.nan, df[column].median())\n",
    "\n",
    "        ### Number of Inventors\n",
    "\n",
    "        df[\"inventors\"] = df[\"inventors\"].apply(lambda x: len(x))\n",
    "\n",
    "        df = df.drop([\"citedby_patents\"], axis=1)\n",
    "\n",
    "        ### Median age of Backward citations\n",
    "\n",
    "        def g(x):\n",
    "\n",
    "            count = len(x[\"cited_patents\"])\n",
    "            if x[\"cited_patents\"][0][\"cited_patent_number\"]:\n",
    "                time = 0\n",
    "                for patent in x[\"cited_patents\"]:\n",
    "                    try:\n",
    "                        time += (x[\"patent_date\"] - datetime.strptime(patent[\"cited_patent_date\"], \"%Y-%m-%d\")).days\n",
    "                    except:\n",
    "                        time+=0\n",
    "                        count-=1\n",
    "                if count == 0:\n",
    "                    return 0\n",
    "                return time / (365*count)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        df[\"age_backward_citation\"] = df.apply(g, axis=1)\n",
    "\n",
    "        # Transforming\n",
    "        df[\"patent_number\"] = df[\"patent_number\"].astype(\"int32\")\n",
    "        df[\"patent_num_combined_citations\"] = df[\"patent_num_combined_citations\"].astype(\"uint16\")\n",
    "        df[\"patent_num_claims\"] = df[\"patent_num_claims\"].astype(\"uint8\")\n",
    "        df[\"inventors\"] = df[\"inventors\"].astype(\"uint8\")\n",
    "        df[\"assignees\"] = df[\"assignees\"].astype(\"uint8\")\n",
    "        df[\"FC3\"] = df[\"FC3\"].astype(\"uint16\")\n",
    "        df[\"FC5\"] = df[\"FC5\"].astype(\"uint16\")\n",
    "        df[\"FC10\"] = df[\"FC10\"].astype(\"uint16\")\n",
    "        df[\"assignees_num_patents\"] = df[\"assignees_num_patents\"].astype(\"int32\")\n",
    "        df[\"num_subclass\"] = df[\"num_subclass\"].astype(\"uint8\")\n",
    "        df[\"age_backward_citation\"] = df[\"age_backward_citation\"].astype(\"float32\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def clean(self, df, year):\n",
    "\n",
    "        # print(df)\n",
    "        # print(df.shape)\n",
    "        # answer = input(\"Do you want to continue?\")\n",
    "        # if answer != \"yes\":\n",
    "        #     return 0\n",
    "\n",
    "        print(\"Cleaning dataframe [...]\")\n",
    "\n",
    "        df[\"cited\"] = df[\"cited_patents\"].apply(lambda x: [patent[\"cited_patent_number\"] for patent in x])\n",
    "\n",
    "        def lookup(criteria):\n",
    "            url = \"https://api.patentsview.org/patents/query\"\n",
    "\n",
    "            data = {\n",
    "                \"q\": {\"patent_number\": criteria},\n",
    "                \"f\": [\"cpc_group_id\",\n",
    "                      \"cpc_subgroup_id\"]\n",
    "            }\n",
    "\n",
    "            resp = requests.post(url, json=data)\n",
    "            json_data = resp.json()\n",
    "\n",
    "            return json_data['patents']\n",
    "\n",
    "\n",
    "        def Herfindahl(x):\n",
    "            tmp = lookup(x[\"cited\"])\n",
    "            if isinstance(tmp,list):\n",
    "                sc1 = list()\n",
    "                sc2 = list()\n",
    "                for cited in tmp:\n",
    "                    for category in cited[\"cpcs\"]:\n",
    "                        sc1.append(category[\"cpc_group_id\"])\n",
    "                        sc2.append(category[\"cpc_subgroup_id\"])\n",
    "\n",
    "                count1 = dict.fromkeys(set(sc1), 0)\n",
    "                count2 = dict.fromkeys(set(sc2), 0)\n",
    "\n",
    "                for classifier in sc1:\n",
    "                    count1[classifier] += 1\n",
    "\n",
    "                for classifier in sc2:\n",
    "                    count2[classifier] += 1\n",
    "\n",
    "                herfindahl1 = 1\n",
    "                for k, v in count1.items():\n",
    "                    herfindahl1 -= (v / len(sc1)) ** 2\n",
    "\n",
    "                herfindahl2 = 1\n",
    "                for k, v in count2.items():\n",
    "                    herfindahl2 -= (v / len(sc2)) ** 2\n",
    "            else:\n",
    "                herfindahl1 = np.nan\n",
    "                herfindahl2 = np.nan\n",
    "\n",
    "            print(\"For row {}, we have a main index of {} and a sub index of {}\".format(x.name,herfindahl1,herfindahl2))\n",
    "\n",
    "            return herfindahl1, herfindahl2\n",
    "\n",
    "        print(\"Calculating Herfindahl\")\n",
    "        df[\"Herfindal_main\"], df[\"Herfindal_sub\"] = zip(*df.apply(Herfindahl, axis=1))\n",
    "\n",
    "        herf_high = df[\"Herfindal_main\"].median()\n",
    "        herf_low = df[\"Herfindal_sub\"].median()\n",
    "\n",
    "        df[\"Herfindal_main\"] = df[\"Herfindal_main\"].replace(np.nan, herf_high)\n",
    "        df[\"Herfindal_sub\"] = df[\"Herfindal_sub\"].replace(np.nan, herf_low)\n",
    "\n",
    "\n",
    "        # Removing uninteresting columns\n",
    "        df = df.drop([\"patent_number\", \"cited_patents\"], axis=1)\n",
    "\n",
    "        # Label encoding\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        diff_labels = list(set([cl for row in df[\"mainclass\"] for cl in row]))\n",
    "        le = LabelEncoder()\n",
    "        le.fit(diff_labels)\n",
    "        df[\"mainclass\"] = df[\"mainclass\"].apply(lambda x: le.transform(x))\n",
    "\n",
    "\n",
    "        # OneHotEncoding\n",
    "        from sklearn.preprocessing import MultiLabelBinarizer\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        test = pd.DataFrame(mlb.fit_transform(df['mainclass']), columns=mlb.classes_, index=df.index)\n",
    "        df = pd.concat([df, test], axis=1, join=\"inner\")\n",
    "\n",
    "        df = df.drop([\"mainclass\",\"cited\"], axis=1)\n",
    "\n",
    "        # Categorising output variables\n",
    "        def h(x):\n",
    "\n",
    "            if x >= 20:\n",
    "                return 3\n",
    "            elif 10 <= x <= 19:\n",
    "                return 2\n",
    "            elif 2 <= x <= 9:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        for col in [\"FC3\", \"FC5\", \"FC10\"]:\n",
    "            df[col] = df[col].apply(lambda x: h(x))\n",
    "\n",
    "        df[\"age_backward_citation\"] = df[\"age_backward_citation\"].fillna(0) ###possibility for median\n",
    "\n",
    "        df.to_pickle(\"df_finished\")\n",
    "        df[df[\"patent_date\"] < datetime(year=year, month=1, day=1)].to_pickle(\"df_train_finished\")\n",
    "\n",
    "        return df,df[df[\"patent_date\"] < datetime(year=year, month=1,day=1)]\n",
    "\n",
    "\n",
    "\n",
    "    def build_ann(self, df, cols, classifier = \"ann\", chained=False):\n",
    "\n",
    "        if len(cols) > 1:\n",
    "            # StratifiedKfoldCrossValidation\n",
    "            df[\"forecast\"] = \"\"\n",
    "            for col in cols:\n",
    "                df[\"forecast\"] += df[col].astype(str)\n",
    "            z = df[\"forecast\"]\n",
    "            df = df.iloc[:, :-1]\n",
    "        else:\n",
    "            z = df[cols[0]]\n",
    "\n",
    "        # Decided whether chained or not\n",
    "        if chained:   ### maybe change??\n",
    "            y = pd.get_dummies(z)\n",
    "        else:\n",
    "            # Otherwise: build output variables\n",
    "            y = pd.DataFrame()\n",
    "            if classifier == \"ann\":\n",
    "                for col in cols:\n",
    "                    one_hot = pd.get_dummies(df[col],prefix=col)\n",
    "                    y = pd.concat([y, one_hot], axis=1)\n",
    "\n",
    "                ### keeping track of output location\n",
    "                for col in cols:\n",
    "                    self.divisions[col] = [i for i, x in enumerate(list(y.columns)) if x.startswith(col)]\n",
    "            else:\n",
    "                y = df[cols[0]]\n",
    "\n",
    "\n",
    "        # Keeping fixed proportions with StratifiedKFold\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True) # change number of splits!!\n",
    "        partitions = []\n",
    "        for train_index, test_index in skf.split(df, z):\n",
    "            partitions.append([list(train_index),list(test_index)])\n",
    "\n",
    "        # Splitting and standardising\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        sc = StandardScaler()\n",
    "\n",
    "        if classifier == \"ann\":\n",
    "            # Building ANN\n",
    "            model = tf.keras.models.Sequential()\n",
    "            model.add(tf.keras.layers.Dense(units=len(df.columns),activation=\"relu\"))\n",
    "            model.add(tf.keras.layers.Dense(units=4,activation=\"relu\"))\n",
    "            model.add(tf.keras.layers.Dense(units=len(y.columns),activation=\"sigmoid\"))\n",
    "            sgd = tf.keras.optimizers.SGD(0.001)\n",
    "            model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "        elif classifier == \"dt\":\n",
    "            from sklearn.tree import DecisionTreeClassifier\n",
    "            model = DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\n",
    "        elif classifier == \"svm\":\n",
    "            from sklearn.svm import SVC\n",
    "            model = SVC(kernel = \"rbf\",random_state=0)\n",
    "        else:\n",
    "            print(\"Incorrect model choice\")\n",
    "            sys.exit()\n",
    "\n",
    "        df = df.drop([\"FC3\",\"FC5\",\"FC10\",\"patent_date\",\"cpcs\"], axis = 1)\n",
    "\n",
    "        for iter in range(len(partitions)):\n",
    "            # split\n",
    "            X_train = df.reindex(index = partitions[iter][0])\n",
    "            y_train = y.reindex(index = partitions[iter][0])\n",
    "            X_test = df.reindex(index = partitions[iter][1])\n",
    "            y_test = y.reindex(index = partitions[iter][1])\n",
    "\n",
    "            # scaling\n",
    "            X_train = sc.fit_transform(X_train)\n",
    "            X_test = sc.transform(X_test)\n",
    "\n",
    "            # Training\n",
    "            if classifier == \"ann\":\n",
    "                model.fit(X_train, y_train, epochs=20)\n",
    "                predict = model.predict(X_test, batch_size=128)\n",
    "                print(iter,predict)\n",
    "                pred_final = pd.DataFrame(columns=y_test.columns, index=y_test.index)\n",
    "                pred_final = pred_final.fillna(0)\n",
    "\n",
    "                for place, patent in enumerate(predict):\n",
    "                    for forecast in self.divisions.values():\n",
    "                        pred_final.iloc[place, patent[forecast[0]:forecast[-1] + 1].argmax() + forecast[0]] = 1\n",
    "\n",
    "                print(iter,pred_final)\n",
    "\n",
    "                ### accuracy score\n",
    "                from sklearn.metrics import accuracy_score\n",
    "                from sklearn.metrics import average_precision_score\n",
    "                for col in pred_final.columns:\n",
    "                    print(col, accuracy_score(pred_final[col], y_test[col]))\n",
    "                    print(average_precision_score(pred_final[col], y_test[col]))\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "                predict = model.predict(X_test)\n",
    "                print(predict)\n",
    "\n",
    "        ### final model training\n",
    "        df = sc.fit_transform(df)\n",
    "        if classifier == \"ann\":\n",
    "            model.fit(df,y, epochs=20)\n",
    "        else:\n",
    "            model.fit(df,y)\n",
    "\n",
    "        return model, sc\n",
    "\n",
    "    def emergingness(self,df, sc, model, cols):\n",
    "\n",
    "        ### Create prediction array\n",
    "        def prediction(x, col_index,name):\n",
    "            if name == \"FC3\": year = 2018\n",
    "            elif name == \"FC5\": year = 2016\n",
    "            elif name == \"FC10\": year = 2011\n",
    "\n",
    "            if x[\"patent_date\"] > datetime(year=year,month=1,day=1):\n",
    "                tmp = x.drop(labels=[\"FC3\",\"FC5\",\"FC10\",\"cpcs\",\"patent_date\"])\n",
    "                output = model.predict(sc.transform(tmp.to_numpy().reshape(1, -1)))[0]\n",
    "                if col_index is None:\n",
    "                    return output\n",
    "                else:\n",
    "                    return output[col_index[0]:col_index[-1]+1].argmax()\n",
    "            else:\n",
    "                return x[name]\n",
    "\n",
    "        if self.divisions:\n",
    "            for k,v in self.divisions.items():\n",
    "                df[k] = df.apply(prediction, col_index = v, name=k, axis = 1)\n",
    "        else:\n",
    "            df[cols[0]] = df.apply(prediction, col_index= None, name=cols[0], axis=1)\n",
    "\n",
    "\n",
    "        ### Get a set of all subgroup IDs\n",
    "        index_classes = list()\n",
    "        def parsing(x):\n",
    "            individual = list()\n",
    "            for patent in x:\n",
    "                subclass = patent[\"cpc_subgroup_id\"].split(\"/\")[0]\n",
    "                index_classes.append(subclass)\n",
    "                individual.append(subclass)\n",
    "            return list(set(individual))\n",
    "\n",
    "        df[\"cpcs\"] = df[\"cpcs\"].apply(parsing)\n",
    "\n",
    "        index_classes = list(set(index_classes))\n",
    "\n",
    "        ### Explode dataframe accoding to subgroup IDs\n",
    "        df = pd.DataFrame({\n",
    "            col: np.repeat(df[col].values, df[\"cpcs\"].str.len())\n",
    "            for col in df.columns.drop(\"cpcs\")}\n",
    "        ).assign(**{\"cpcs\": np.concatenate(df[\"cpcs\"].values)})[df.columns]\n",
    "\n",
    "        ### Create final time series emergingness matrix\n",
    "        value = {0: 1, 1: 3, 2: 5, 3: 10}\n",
    "\n",
    "        def output_emerging(name):\n",
    "            years = np.arange(1986, 2021, 1)\n",
    "            final = pd.DataFrame(index=index_classes, columns=years)\n",
    "\n",
    "            for subclass in index_classes:\n",
    "                for year in years:\n",
    "                    summation = 0\n",
    "                    for k,v in value.items():\n",
    "                        cluster = df[(df[\"patent_date\"] >= datetime(year=year,month=1,day=1)) & (df[\"patent_date\"] <= datetime(year=year,month=12,day=31)) & (df[\"cpcs\"] == subclass) & (df[name] == k)]\n",
    "                        total = df[(df[\"patent_date\"] >= datetime(year=year,month=1,day=1)) & (df[\"patent_date\"] <= datetime(year=year,month=12,day=31)) & (df[\"cpcs\"] == subclass)]\n",
    "                        summation += len(cluster.index) / (len(total.index) if len(total.index != 0) else 1000000) * v\n",
    "                    final[year][subclass] = summation\n",
    "\n",
    "            final = final[final.index.str.startswith(self.category)]\n",
    "            print(final)\n",
    "            ### Normalising the data\n",
    "            column_maxes = final.max()\n",
    "            df_max = column_maxes.max()\n",
    "            if df_max != 0:\n",
    "                final = final / df_max\n",
    "            print(final)\n",
    "\n",
    "            ### Time-series evolution: N - N-1\n",
    "            final_diff = final.diff(axis=1)\n",
    "            print(final_diff)\n",
    "\n",
    "        for timeframe in cols:\n",
    "            output_emerging(timeframe)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prep_csv()\n",
    "    start = Analysis()\n",
    "    df = start.data(cpc = \"H04W48/18\", name=\"data_trial\")\n",
    "    df = pd.read_pickle(\"data_trial\")\n",
    "    df,df_train = start.clean(df, 2018)\n",
    "    ann_model, sc = start.build_ann(df_train, [\"FC3\",\"FC5\",\"FC10\"], classifier = \"ann\", chained=False)\n",
    "    start.emergingness(df, sc, ann_model,[\"FC3\"])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
